{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "In this notebook we train a SGDClassifier on the embeddings of positive and negative words. Then given a sentence the sentiment analysis is done by classifying each word in the sentence as positive or negative. The final outcome is produced by aggregating the resulting sentiment of each word in this sentence."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "z8ZfbE-57xLC",
        "outputId": "1f1ba9e3-aa56-49da-f86a-4788dd474699"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-zv0GMfe8Z5X",
        "outputId": "c52d7530-44eb-4a90-c584-6ca97c93dc59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "CXxmJgLw8iXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(filename):\n",
        "    labels = []\n",
        "    rows = []\n",
        "    with open(filename, encoding='utf-8') as infile:\n",
        "        for i, line in tqdm(enumerate(infile)):\n",
        "            items = line.rstrip().split(' ')\n",
        "            if len(items) == 2:\n",
        "                # This is a header row giving the shape of the matrix\n",
        "                continue\n",
        "            labels.append(items[0])\n",
        "            values = np.array([float(x) for x in items[1:]], 'f')\n",
        "            rows.append(values)\n",
        "    \n",
        "    arr = np.vstack(rows)\n",
        "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
        "\n",
        "# embeddings_big = load_embeddings('glove.42B.300d.txt') # big glove\n",
        "embeddings_small = load_embeddings('glove.6B.50d.txt') # small glove\n",
        "embeddings.shape"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400000it [00:07, 54312.08it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "(400000, 50)"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "y8dxdBXk8qZL",
        "outputId": "5abfb2cc-b809-43fc-ca1d-c71fa91f9d14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings = embeddings_big.copy()\n",
        "embeddings = embeddings_small.copy()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "3VNGaVQx_AZE",
        "outputId": "2e619300-cbcd-435a-e44a-80cf4a599b4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_words = pd.read_csv('moodstock_data/Positive-words.tsv', header=None, sep='\\t')\n",
        "neg_words = pd.read_csv('moodstock_data/Negative-words.tsv', header=None, sep='\\t')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "l-tmnHVx8svZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_words[0] = pos_words[0].str.lower()\n",
        "neg_words[0] = neg_words[0].str.lower()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "BU3OuFH6BTiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_vectors = embeddings.loc[pos_words.values[:,0]].dropna()\n",
        "neg_vectors = embeddings.loc[neg_words.values[:,0]].dropna()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "COPUIbRJ-BEG",
        "outputId": "7f4ef20c-a3b0-4716-b4a5-b9bfd12ec354"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = pd.concat([pos_vectors, neg_vectors])\n",
        "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
        "labels = list(pos_vectors.index) + list(neg_vectors.index)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "P_3yMUHW_aTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
        "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=12)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "mTa5GS50Cs2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD Classifier"
      ],
      "metadata": {
        "id": "mzREzGnhSmUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import SGDClassifier"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "kNRD-77UCvYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SGDClassifier(loss='log', random_state=32, max_iter=100)\n",
        "model.fit(vectors, targets)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=100,\n              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n              random_state=32, shuffle=True, tol=0.001, validation_fraction=0.1,\n              verbose=0, warm_start=False)"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "eFmZC82vC-VN",
        "outputId": "f2c4125e-4a64-40f2-cc88-080603a7918f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = model.predict(test_vectors)\n",
        "accuracy_score(y_true=test_targets, y_pred=y_hat)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "0.9507575757575758"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XLJswDdxDDZG",
        "outputId": "5ee4bfda-6ce6-4cfc-9115-7afeee229606"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model = pickle.load(open('sentiment_analisys.sav', 'rb'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qrVtLz-b9fZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
        "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
        "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
        "# expression that manages to extract something very much like words from text.\n",
        "\n",
        "def vecs_to_sentiment(vecs):\n",
        "    # predict_log_proba gives the log probability for each class\n",
        "    predictions = model.predict_log_proba(vecs)\n",
        "    # To see an overall positive vs. negative classification in one number,\n",
        "    # we take the log probability of positive sentiment minus the log\n",
        "    # probability of negative sentiment.\n",
        "    return predictions[:, 1] - predictions[:, 0]\n",
        "\n",
        "\n",
        "def words_to_sentiment(words):\n",
        "    vecs = embeddings.loc[words].dropna()\n",
        "    log_odds = vecs_to_sentiment(vecs)\n",
        "    return log_odds\n",
        "\n",
        "\n",
        "def text_to_sentiment(tokens, print_sent=False):\n",
        "    sentiments = words_to_sentiment(tokens)\n",
        "    mean = sentiments.mean()\n",
        "\n",
        "    if print_sent:\n",
        "        print(mean)\n",
        "\n",
        "    if mean >= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "R1PMMGqCDFId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sentiment_norm(tokens, print_sent=False):\n",
        "    sentiments = words_to_sentiment(tokens)\n",
        "    pos = sum([1 for s in sentiments if s >= 0])\n",
        "    neg = sum([1 for s in sentiments if s < 0])\n",
        "\n",
        "    if print_sent:\n",
        "    print(sentiments)\n",
        "\n",
        "    if pos >= neg:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "bqpcoyCBGAcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess\n"
      ],
      "metadata": {
        "id": "bZZ_hD1cpIK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "import emoji\n",
        "import datefinder\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_emojies(word):\n",
        "    return emoji.demojize(word)\n",
        "\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def remove_apostrophe(word):\n",
        "    return decontracted(word)\n",
        "\n",
        "def substitute_company_names(phrase):\n",
        "    phrase = re.sub(r\"\\$[A-Za-z]+\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"Netflix\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"Disney\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"Apple\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"Alphabet\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"Tesla\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"Facebook\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"netflix\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"disney\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"baba\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"apple\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"alphabet\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"tesla\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"facebook\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"SPY\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"spy\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"spy500\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"S&P\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"S&P500\", \"ORG\", phrase)\n",
        "    phrase = re.sub(\"s&p500\", \"ORG\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def substitute_date(phrase):\n",
        "    matches = datefinder.find_dates(phrase,source=True,index=True)\n",
        "    indices=[]\n",
        "    number_dates=0\n",
        "    number_dates2=0\n",
        "    y=0\n",
        "    try:\n",
        "        for match in matches:\n",
        "            indices.append(match[2])\n",
        "        for i in indices:\n",
        "            if number_dates==0:\n",
        "                phrase = phrase[:i[0]] + \"  Date  \" + phrase[i[1]:]\n",
        "            else:\n",
        "                phrase = phrase[:i[0]-number_dates2+len(\"  Date  \")*y] + \"  Date  \" + phrase[i[1]-number_dates2+len(\"  Date  \")*y:]\n",
        "                \n",
        "            number_dates=+i[1]-i[0]\n",
        "            number_dates2=number_dates+number_dates2\n",
        "            y+=1\n",
        "    finally:\n",
        "        return phrase\n",
        "\n",
        "def substitute_prices(word):\n",
        "    return re.sub(r\"\\$[0-9]+\", \"price\", word)\n",
        "\n",
        "def substitute_entities(phrase):\n",
        "    nlp = en_core_web_sm.load()\n",
        "    doc = nlp(phrase)\n",
        "    newString = phrase\n",
        "    for e in reversed(doc.ents):\n",
        "        start = e.start_char\n",
        "        end = start + len(e.text)\n",
        "        newString = newString[:start] + e.label_.lower() + newString[end:]\n",
        "    return newString\n",
        "\n",
        "def remove_entities(words):\n",
        "    entities=['cardinal', 'date', 'event', 'price', 'org', 'fac', 'gpe', 'language', 'law', 'loc', 'money', 'norp', 'ordinal', 'percent', 'person', 'product', 'quantity', 'time', 'work_of_art']\n",
        "    return [i for i in words if i not in entities]\n",
        "\n",
        "def remove_single_letters(words):\n",
        "    return [i for i in words if len(i) > 1]\n",
        "\n",
        "def remove_hyperlink(word):\n",
        "    return  re.sub(r\"http\\S+\", \"\", word)\n",
        "\n",
        "\n",
        "def to_lower(word):\n",
        "    result = word.lower()\n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_number(word):\n",
        "    result = re.sub(r'\\d+', '', word)\n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_punctuation(word):\n",
        "    # result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "    return re.sub(r'[^A-Za-z]', ' ', word)\n",
        "\n",
        "\n",
        "def remove_whitespace(word):\n",
        "    result = word.strip()\n",
        "    return result\n",
        "\n",
        "\n",
        "def replace_newline(word):\n",
        "    return word.replace('\\n','')\n",
        "\n",
        "def remove_stop_words(words):\n",
        "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
        "    return result\n",
        "\n",
        "def word_stemmer(words):\n",
        "    return [stemmer.stem(o) for o in words]\n",
        "\n",
        "def word_lemmatizer(words):\n",
        "    return [lemmatizer.lemmatize(o) for o in words]\n",
        "\n",
        "def filtering_pipeline(sentence):\n",
        "    cleaning_utils = [substitute_date,\n",
        "                      to_lower,\n",
        "                      substitute_company_names,\n",
        "                      substitute_prices,\n",
        "                      remove_hyperlink,\n",
        "                      remove_number,\n",
        "                      remove_emojies,\n",
        "                      remove_apostrophe,\n",
        "                      remove_punctuation,\n",
        "                      substitute_entities,\n",
        "                      replace_newline,\n",
        "                      remove_whitespace]\n",
        "\n",
        "    cleaning_tokens = [remove_stop_words,\n",
        "                       word_lemmatizer]\n",
        "    for o in cleaning_utils:\n",
        "        sentence = o(sentence)\n",
        "    sentence = word_tokenize(sentence)\n",
        "    for i in cleaning_tokens:\n",
        "        sentence = i(sentence)\n",
        "    return sentence\n",
        "\n",
        "def sentiment_analysis_pipeline(tokens):\n",
        "    tokens = remove_entities(tokens)\n",
        "    return remove_single_letters(tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "cVyhB59GaUDS",
        "outputId": "d48aa4dc-eb68-47b7-ff6c-7172eb01fb64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Tweets\n"
      ],
      "metadata": {
        "id": "XrFJVK1MpQmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = pd.read_csv('moodstock_data/MoodstockDataset-Sentiment-9839.csv', index_col=1)\n",
        "tweets_df.loc[tweets_df['Label'] == -2, 'Sentiment'] *= -1\n",
        "tweets_df_no_spam = tweets_df.drop(tweets_df.loc[tweets_df['Label'] == -1].index, axis=0)\n",
        "binary_tweets = tweets_df_no_spam.drop(tweets_df.loc[tweets_df['Sentiment'] == 0].index, axis=0, errors='ignore')\n",
        "binary_tweets.drop(9959, axis=0,inplace=True)\n",
        "\n",
        "# tweets_df_all_agree = pd.read_csv('moodstock_data/Sentences_AllAgree.txt', sep='@', encoding='latin-1', header=None)\n",
        "# tweets_df_all_agree.columns = ['Text','Sentiment']\n",
        "# tweets_df_all_agree.loc[tweets_df_all_agree['Sentiment'] == 'positive', 'Sentiment'] = 1\n",
        "# tweets_df_all_agree.loc[tweets_df_all_agree['Sentiment'] == 'negative', 'Sentiment'] = -1\n",
        "# tweets_df_all_agree.tail()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "gD1E1s9xVRLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_tweets.iloc[92]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 62,
          "data": {
            "text/plain": "Label                                           0\nText         $BABA\\nshredddd\\nover $200 explosion\nSentiment                                       1\nName: 9961, dtype: object"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "VxvsaJ72EesE",
        "outputId": "a7e1747d-d253-44c4-c549-35ac0d0fcc88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_tweets = binary_tweets['Text'].values\n",
        "y_tweets = binary_tweets['Sentiment'].values\n",
        "\n",
        "x_tweets = [filtering_pipeline(o) for o in x_tweets]\n",
        "x_tweets = [sentiment_analysis_pipeline(o) for o in x_tweets]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "jr3mfVj9YoRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on tweets SGD"
      ],
      "metadata": {
        "id": "Z3Z6LhMjTVJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = []\n",
        "for i,sample in enumerate(x_tweets):\n",
        "    y_hat.append(text_to_sentiment_norm(sample))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tWUf94S2x0bG",
        "outputId": "43e9cf28-2d97-416b-de46-8244f65a0f36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_hat)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 63,
          "data": {
            "text/plain": "92"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Z-x2HtKdyTgS",
        "outputId": "28c5a687-5b5a-4bbe-c648-84999eb6171a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_true=y_tweets, y_pred=y_hat)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 68,
          "data": {
            "text/plain": "0.8214285714285714"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "v6-D29Sp8b4P",
        "outputId": "e845d944-0208-40c9-a4ee-b1dae4bd71f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "print(precision_score(y_true=y_tweets, y_pred=y_hat))\n",
        "recall_score(y_true=y_tweets, y_pred=y_hat)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.925531914893617\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 70,
          "data": {
            "text/plain": "0.87"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "NchDCdEk5fak",
        "outputId": "1fdd3117-d239-4dae-f323-ee569708d2a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtering_pipeline('$BABA Stock Increased 2.3% to 199.10. The Largest Options Open Interest is on the 17-Jan-20 200 Call with 35,672(OI)')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 305,
          "data": {
            "text/plain": "['ORG', 'stock', 'increased', 'date', 'largest', 'option', 'open', 'jan', 'oi']"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "HonEuu7v-ety",
        "outputId": "a1f3c983-2422-470a-b6e2-8dd9e752af9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# substitute_date('17/11/20 200')\n",
        "matches = datefinder.find_dates('created 17-Jan-20 by ACME Inc. and associates.')\n",
        "for match in matches:\n",
        "    print(match)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020-01-17 00:00:00\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "wpvsYuoJyYdE",
        "outputId": "a5f86c73-18b2-4393-c0ee-5a9e47ecafd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.where(y_hat != y_tweets)[0]\n",
        "for i in idx:\n",
        "    print(binary_tweets.iloc[i][['Text', 'Sentiment']].values[0])\n",
        "    print(binary_tweets.iloc[i][['Text', 'Sentiment']].values[1])\n",
        "    print(np.array(x_tweets)[i])\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Qb-sVm9410rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_sentiment_norm(['increased', 'far', 'nearing'], True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           sentiment\n",
            "increased   0.822870\n",
            "far         0.904636\n",
            "nearing     0.670365\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 354,
          "data": {
            "text/plain": "1"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "C8TBHDHeAKBV",
        "outputId": "8c765efd-9398-4452-f568-ffc63286a527"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_sentiment_norm(['very','angry'], True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['very', 'angry']\n",
            "[  3.87275611 -11.04012794]\n",
            "[  3.87275611 -11.04012794]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": "1"
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "eKj7aGS-8DXT",
        "outputId": "612d6423-610a-42cd-d134-9940a2f4a521"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Z3Z6LhMjTVJ3"
      ],
      "name": "moodstock_sentiment",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.26.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}