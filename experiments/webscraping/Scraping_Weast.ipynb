{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import time\n",
    "#import pymongo\n",
    "import re\n",
    "\n",
    "\n",
    "with open('consumer_key.txt', 'r') as f:\n",
    "    consumer_key =  f.read()\n",
    "f.closed\n",
    "\n",
    "with open('consumer_secret.txt', 'r') as f:\n",
    "    consumer_secret = f.read()\n",
    "f.closed\n",
    "\n",
    "with open('access_key.txt', 'r') as f:\n",
    "    access_key = f.read()\n",
    "f.closed\n",
    "\n",
    "with open('access_secret.txt', 'r') as f:\n",
    "     access_secret = f.read()\n",
    "f.closed\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipedia as wp\n",
    "import numpy as np\n",
    " \n",
    "#From wiki we get a list of tickers, to pass to the scraper\n",
    "html = wp.page(\"List of S&P 500 companies\").html().encode(\"UTF-8\")\n",
    "df = pd.read_html(html)[0]\n",
    "\n",
    "\n",
    "Sector_dic={}\n",
    "df = df[['Symbol', 'GICS Sector']]\n",
    "stocks_to_be_scraped = np.array(df['Symbol'].to_list()).reshape((505,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "stocks_list = []\n",
    "texts = [] \n",
    "#Create a text list to avoid duplicate in the stocks_list\n",
    "\n",
    "#here onother list if we want to select certain tickers\n",
    "#stocks_to_be_scraped = [['FB', 'BRK', 'JNJ'], ['XOM', 'BAC', 'WMT'], ['WFC', 'AMZN', 'RDS'], ['PG', 'NFLX', 'DIS'], ['TSLA', 'BABA', 'GM'], ['GE', 'BA',\n",
    "#'BUD'], ['CVX', 'UNH', 'PFE'], ['OTC', 'CHL', 'SE', 'FCA'], [\"APPL\", \"GOOGL\", 'MSFT']]\n",
    "\n",
    "def scrape_for_stock(stock):\n",
    "    NUM_TWEETS_PER_FETCH = 100000\n",
    "    search_query = \"${} -filter:retweets\".format(stock)\n",
    "    for tweet in tweepy.Cursor(api.search,\n",
    "                               q=search_query,\n",
    "                               count=NUM_TWEETS_PER_FETCH,\n",
    "                               wait_on_rate_limit = True,\n",
    "                               # Contains some useful info in the entities like what are the other stocks\n",
    "                               # in the tweet so we might use it in the future\n",
    "                               include_entities=True,\n",
    "                               result_type='recent',  # fetch only the most recent tweets\n",
    "                               lang='en',  # our sentiment model works with English only\n",
    "                               tweet_mode=\"extended\"\n",
    "                               ).items(NUM_TWEETS_PER_FETCH):\n",
    "        #add this if to avoid duplicates\n",
    "        if tweet.full_text not in texts and len(re.findall(r'\\$[A-Z]+', tweet.full_text)) == 1:\n",
    "            #print(tweet.full_text)\n",
    "            tweet_data = {\n",
    "            'text': tweet.full_text,\n",
    "            'tweet_id': tweet.id_str,\n",
    "            'created_at': tweet.created_at,\n",
    "            'username': tweet.user.name,\n",
    "            'retweets': tweet.retweet_count\n",
    "            }\n",
    "        \n",
    "            texts.append(tweet.full_text)\n",
    "            \n",
    "            stocks_list.append(tweet_data)\n",
    "    return stocks_list\n",
    "   \n",
    "for i in stocks_to_be_scraped:\n",
    "    for stock in i:\n",
    "        print('....I am fetching this stock ${}....'.format(stock))\n",
    "        scrape_for_stock(stock)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stocks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I add the feature ticker\n",
    "for i in stocks_list:\n",
    "    i['ticker'] = re.findall(r'\\$[A-Z]+', i['text'])[0]\n",
    "\n",
    "stocks_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "try:\n",
    "    # create a credentials.txt file in this folder:\n",
    "    # One line: Database DSN <driver>://<username>:<password>@<host>:<port>/<database>\n",
    "#           In our case the DSN is: mongodb://heroku_7wnll0gx:9n3tqoacbtmlfq6eiqati7uhj7@ds343718.mlab.com:43718/heroku_7wnll0gx\t\t  \n",
    "    with open(\"credentials.txt\", 'r') as f:\n",
    "        database_dns = f.read().splitlines()\n",
    "\n",
    "# Note that the retryWrites is set to false as heroku's mlab integration does not support them\n",
    "    conn=pymongo.MongoClient(database_dns,retryWrites=False)\n",
    "    print (\"Connected successfully!!!\")\n",
    "    \n",
    "except pymongo.errors.ConnectionFailure as e:\n",
    "    print (\"Could not connect to MongoDB: %s\" % e) \n",
    "\n",
    "# we have to explicitly specify the database name again which in our case is:\n",
    "db = conn['heroku_7wnll0gx']\n",
    "collection = db['tweets_list_10000_3']\n",
    "collection.insert_many(stocks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
